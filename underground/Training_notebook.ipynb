{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d37bcf-67ee-4caa-a6ee-0231f04c79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from glob import glob\n",
    "import tifffile\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ad0e8-9bd1-4100-85bf-0af33b592228",
   "metadata": {},
   "source": [
    "### Creating a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a192c5-22f2-4194-907b-0995caa1e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbryoNucleiDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 crop_size,\n",
    "                ):\n",
    "        \n",
    "        # using root_dir, split and mask create a path to files and sort it \n",
    "        self.mask_files = natsorted(glob(os.path.join(root_dir, 'masks', 'masks*.tif'))) # load mask files into sorted list\n",
    "        self.raw_files = natsorted(glob(os.path.join(root_dir, 'raw_files', 'raw*.tif'))) # load image files into sorted list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.raw_files)\n",
    "\n",
    "    def get_centroids(self, mask):\n",
    "        ids = np.unique(mask)\n",
    "        ids = ids[1:]\n",
    "        centroids = []\n",
    "        for id in ids:\n",
    "            y,x = np.where(mask == id)\n",
    "            xm = int(np.mean(x))\n",
    "            ym = int(np.mean(y))\n",
    "            centroid = (ym, xm)\n",
    "            centroids.append(centroid)\n",
    "        \n",
    "        centroids = np.array(centroids)\n",
    "        return centroids\n",
    "    \n",
    "    def crop_top_left(self, coord): \n",
    "        y,x = coord\n",
    "        y_top_left = int(y-(self.crop_size//2))\n",
    "        x_top_left = int(x-(self.crop_size//2))\n",
    "        return y_top_left, x_top_left\n",
    "    \n",
    "    def get_masked_crop(self, raw, mask, batch_size):\n",
    "        crops_raw = []\n",
    "        crops_mask = []\n",
    "        centroids = get_centroids(mask)\n",
    "        \n",
    "        while len(crops_masks) < batch_size: \n",
    "            centroid = random.choice(centroids)\n",
    "            y_top_left, x_top_left = self.crop_top_left(centroid)\n",
    "            crop_mask = mask[y_top_left:y_top_left+self.crop_size, x_top_left:x_top_left+self.crop_size]\n",
    "            if crop_mask.shape == (self.crop_size, self.crop_size):\n",
    "                crop_mask = (crop_mask == crop_mask[int(self.crop_size//2), int(self.crop_size//2)])\n",
    "                crop_raw = raw[x_top_left:x_top_left+self.crop_size, y_top_left:y_top_left+self.crop_size]\n",
    "                crops_raw.append(crop_raw*crop_mask)\n",
    "                crops_mask.append(crop_mask)\n",
    "        return crops_raw, crops_mask \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        raw_file = self.raw_files[idx] \n",
    "        mask_file = self.mask_files[idx] \n",
    "        \n",
    "        raw = imread(raw_file) # load raw to numpy array\n",
    "        mask = imread(mask_file) # load mask to numpy array\n",
    "\n",
    "        # from (H, W) mask extract (B, h, h)\n",
    "        crops_raw, crops_mask = get_masked_crops(raw, mask)\n",
    "\n",
    "        # need to cast to float32\n",
    "        crops_mask = (crops_mask !=0).astype(np.float32)\n",
    "        crops_raw = (crops_raw != 0).astype(np.float32)\n",
    "        \n",
    "        # add channel dimensions to comply with pytorch standard (B, C, H, W) \n",
    "        crops_raw = np.expand_dims(crops_raw, axis=1)\n",
    "        crops_mask = np.expand_dims(crops_mask, axis=1)\n",
    "        \n",
    "        return crops_raw, crops_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabeb4b-2073-4e51-9033-cdfb1e8d22ce",
   "metadata": {},
   "source": [
    "### Creating Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "015211c4-1793-4b3c-a578-a68d59e09fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            downsampling_factors,\n",
    "            fmaps,\n",
    "            fmul,\n",
    "            kernel_size=3):\n",
    "\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        out_channels = in_channels\n",
    "\n",
    "        encoder = []\n",
    "\n",
    "        for downsampling_factor in downsampling_factors:\n",
    "\n",
    "            encoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "            encoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "            encoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        fmaps,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "            encoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "            encoder.append(\n",
    "                    torch.nn.MaxPool2d(downsampling_factor))\n",
    "\n",
    "            in_channels = fmaps\n",
    "\n",
    "            fmaps = fmaps * fmul\n",
    "\n",
    "        fmaps_bottle = fmaps\n",
    "\n",
    "        encoder.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels,\n",
    "                fmaps_bottle,\n",
    "                kernel_size))\n",
    "        encoder.append(\n",
    "            torch.nn.ReLU(inplace=True))\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*encoder)\n",
    "\n",
    "        decoder = []\n",
    "\n",
    "        fmaps = in_channels\n",
    "\n",
    "        decoder.append(\n",
    "            torch.nn.Conv2d(\n",
    "                fmaps_bottle,\n",
    "                fmaps,\n",
    "                kernel_size))\n",
    "        decoder.append(\n",
    "            torch.nn.ReLU(inplace=True))\n",
    "\n",
    "        for downsampling_factor in downsampling_factors[::-1]:\n",
    "\n",
    "            fmaps = in_channels / fmul\n",
    "\n",
    "            decoder.append(\n",
    "                torch.nn.Upsample(\n",
    "                    scale_factor=downsampling_factor,\n",
    "                    mode='trilinear'))\n",
    "            decoder.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    fmaps,\n",
    "                    kernel_size))\n",
    "            decoder.append(\n",
    "                torch.nn.ReLU(inplace=True))\n",
    "            decoder.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    fmaps,\n",
    "                    fmaps,\n",
    "                    kernel_size))\n",
    "            decoder.append(\n",
    "                torch.nn.ReLU(inplace=True))\n",
    "\n",
    "            in_channels = fmaps\n",
    "\n",
    "        decoder.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size))\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(*decoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        enc = self.encoder(x)\n",
    "\n",
    "        dec = self.decoder(enc)\n",
    "\n",
    "        return enc, dec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d250cf0-04ac-4065-9776-fc2cc0bea19a",
   "metadata": {},
   "source": [
    "### Training Time ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86f9571c-b030-413b-aa2e-b10f2110a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying params for training\n",
    "batch_size = 32\n",
    "crop_size = 156\n",
    "num_epochs = 50\n",
    "model_depth = 3\n",
    "root_dir = '/mnt/efs/shared_data/instance_no_gt/20230830_TIF_cellpose_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56596df7-0a9a-4ced-a71d-0d030928261b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(args):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def train(args):\n",
    "    print(args)\n",
    "    \n",
    "    # create train dataset\n",
    "    dataset = EmbryoNucleiDataset(root_dir, crop_size)\n",
    "\n",
    "    \n",
    "\n",
    "    # create val dataset\n",
    "\n",
    "\n",
    "    # create model\n",
    "    model = Autoencoder(in_channels=1, downsampling_factors=[2,2,2],\n",
    "        fmaps=32, fmul=2, kernel_size = 3)\n",
    "\n",
    "    # create loss object\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "\n",
    "    # create optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(dataset, model, epoch, optimizer, loss_function)\n",
    "\n",
    "def train_epoch(dataset,model,epoch,optimizer,loss_function):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    loss_list = []    \n",
    "    for batch_id, (raw, mask) in enumerate(dataset):\n",
    "        raw = raw.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # apply model and calculate loss\n",
    "        prediction = model(raw)\n",
    "        loss = loss_function(prediction, raw)\n",
    "        loss_list.append(loss.item())\n",
    "        # backpropagate the loss and adjust the parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Loss at Epoch {epoch} is {loss_list.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02e25b0d-7380-440b-8daf-c7c11f9f1653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:underground-env]",
   "language": "python",
   "name": "conda-env-underground-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
