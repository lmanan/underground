{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d37bcf-67ee-4caa-a6ee-0231f04c79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from glob import glob\n",
    "import tifffile\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ad0e8-9bd1-4100-85bf-0af33b592228",
   "metadata": {},
   "source": [
    "### Creating a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a192c5-22f2-4194-907b-0995caa1e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbryoNucleiDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 crop_size,\n",
    "                ):\n",
    "        \n",
    "        # using root_dir, split and mask create a path to files and sort it \n",
    "        self.mask_files = sorted(glob(os.path.join(root_dir, 'masks', 'masks*.tif'))) # load mask files into sorted list\n",
    "        self.raw_files = sorted(glob(os.path.join(root_dir, 'raw_files', 'raw*.tif'))) # load image files into sorted list\n",
    "        self.crop_size = crop_size\n",
    "        self.batch_size = 32\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.raw_files)\n",
    "        #return 10\n",
    "\n",
    "    def get_centroids(self, mask):\n",
    "        ids = np.unique(mask)\n",
    "        ids = ids[1:]\n",
    "        centroids = []\n",
    "        for id in ids:\n",
    "            y,x = np.where(mask == id)\n",
    "            xm = int(np.mean(x))\n",
    "            ym = int(np.mean(y))\n",
    "            centroid = (ym, xm)\n",
    "            centroids.append(centroid)\n",
    "        \n",
    "        centroids = np.array(centroids)\n",
    "        return centroids\n",
    "    \n",
    "    def crop_top_left(self, coord): \n",
    "        y, x = coord\n",
    "        y_top_left = int(y-(self.crop_size//2))\n",
    "        x_top_left = int(x-(self.crop_size//2))\n",
    "        return y_top_left, x_top_left\n",
    "    \n",
    "    def get_masked_crops(self, raw, mask):\n",
    "        crops_raw = []\n",
    "        crops_mask = []\n",
    "        centroids = self.get_centroids(mask)\n",
    "        \n",
    "        while len(crops_mask) < self.batch_size:\n",
    "            centroid = random.choice(centroids)\n",
    "            y_top_left, x_top_left = self.crop_top_left(centroid)\n",
    "            crop_mask = mask[y_top_left:y_top_left+self.crop_size, x_top_left:x_top_left+self.crop_size]\n",
    "            if crop_mask.shape == (self.crop_size, self.crop_size):\n",
    "                crop_mask = (crop_mask == crop_mask[int(self.crop_size//2), int(self.crop_size//2)])\n",
    "                crop_raw = raw[y_top_left:y_top_left+self.crop_size, x_top_left:x_top_left+self.crop_size]\n",
    "                crops_raw.append(crop_raw*crop_mask)\n",
    "                crops_mask.append(crop_mask)\n",
    "        return np.array(crops_raw), np.array(crops_mask)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        num_objects = 0\n",
    "        while num_objects<self.batch_size:\n",
    "            idx = np.random.randint(len(self.raw_files))\n",
    "            mask_file = self.mask_files[idx] \n",
    "            mask = tifffile.imread(mask_file) \n",
    "            ids = np.unique(mask)\n",
    "            ids = ids[ids!=0] # skip b.g.\n",
    "            num_objects = len(ids)\n",
    "            \n",
    "        #print(f\"Current index is {idx}\")\n",
    "        raw_file = self.raw_files[idx] \n",
    "        #mask_file = self.mask_files[idx] \n",
    "        #print(f\"Crops are being extracted from {raw_file} file currently\")\n",
    "        raw = tifffile.imread(raw_file) # load raw to numpy array\n",
    "        #mask = tifffile.imread(mask_file) # load mask to numpy array\n",
    "        \n",
    "        # from (H, W) mask extract (B, h, h)\n",
    "        crops_raw, crops_mask = self.get_masked_crops(raw, mask)\n",
    "        #print(f\"Crops raw have shape {crops_raw.shape}. Crops Mask have shape {crops_mask.shape}\")\n",
    "        # need to cast to float32\n",
    "        \n",
    "        crops_mask = (crops_mask !=0).astype(np.float32)\n",
    "        crops_raw = (crops_raw.astype(np.float32))/65535\n",
    "        \n",
    "        # add channel dimensions to comply with pytorch standard (B, C, H, W) \n",
    "        crops_raw = np.expand_dims(crops_raw, axis=1)\n",
    "        crops_mask = np.expand_dims(crops_mask, axis=1)\n",
    "        \n",
    "        return crops_raw, crops_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabeb4b-2073-4e51-9033-cdfb1e8d22ce",
   "metadata": {},
   "source": [
    "### Creating Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015211c4-1793-4b3c-a578-a68d59e09fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            downsampling_factors,\n",
    "            fmaps,\n",
    "            fmul,\n",
    "            kernel_size=3):\n",
    "\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        out_channels = in_channels\n",
    "\n",
    "        encoder = []\n",
    "\n",
    "        for downsampling_factor in downsampling_factors:\n",
    "\n",
    "            encoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "            encoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "            encoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        fmaps,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "            encoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "            encoder.append(\n",
    "                    torch.nn.MaxPool2d(downsampling_factor))\n",
    "\n",
    "            in_channels = fmaps\n",
    "\n",
    "            fmaps = fmaps * fmul\n",
    "\n",
    "        fmaps_bottle = fmaps\n",
    "\n",
    "        encoder.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels,\n",
    "                fmaps_bottle,\n",
    "                kernel_size))\n",
    "        encoder.append(\n",
    "            torch.nn.ReLU(inplace=True))\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*encoder)\n",
    "\n",
    "        decoder = []\n",
    "\n",
    "        fmaps = in_channels\n",
    "\n",
    "        decoder.append(\n",
    "            torch.nn.Conv2d(\n",
    "                fmaps_bottle,\n",
    "                fmaps,\n",
    "                kernel_size))\n",
    "        decoder.append(\n",
    "            torch.nn.ReLU(inplace=True))\n",
    "\n",
    "        for downsampling_factor in downsampling_factors[::-1]:\n",
    "\n",
    "            fmaps = in_channels // fmul\n",
    "\n",
    "            decoder.append(\n",
    "                torch.nn.Upsample(\n",
    "                    scale_factor=downsampling_factor,\n",
    "                    mode='bilinear'))\n",
    "            decoder.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    fmaps,\n",
    "                    kernel_size))\n",
    "            decoder.append(\n",
    "                torch.nn.ReLU(inplace=True))\n",
    "            decoder.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    fmaps,\n",
    "                    fmaps,\n",
    "                    kernel_size))\n",
    "            decoder.append(\n",
    "                torch.nn.ReLU(inplace=True))\n",
    "\n",
    "            in_channels = fmaps\n",
    "\n",
    "        decoder.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size))\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(*decoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        enc = self.encoder(x)\n",
    "\n",
    "        dec = self.decoder(enc)\n",
    "\n",
    "        return enc, dec\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d250cf0-04ac-4065-9776-fc2cc0bea19a",
   "metadata": {},
   "source": [
    "### Training Time ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f9571c-b030-413b-aa2e-b10f2110a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying params for training\n",
    "batch_size = 64\n",
    "crop_size = 156\n",
    "num_epochs = 50\n",
    "model_depth = 1\n",
    "downsampling_factor = 2\n",
    "root_dir = '/mnt/efs/shared_data/instance_no_gt/20230830_TIF_cellpose_test/'\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35176614-4dbb-42ca-ade7-f5ba5e37a2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 154, 154]             320\n",
      "              ReLU-2         [-1, 32, 154, 154]               0\n",
      "            Conv2d-3         [-1, 32, 152, 152]           9,248\n",
      "              ReLU-4         [-1, 32, 152, 152]               0\n",
      "         MaxPool2d-5           [-1, 32, 76, 76]               0\n",
      "            Conv2d-6           [-1, 64, 74, 74]          18,496\n",
      "              ReLU-7           [-1, 64, 74, 74]               0\n",
      "            Conv2d-8           [-1, 32, 72, 72]          18,464\n",
      "              ReLU-9           [-1, 32, 72, 72]               0\n",
      "         Upsample-10         [-1, 32, 144, 144]               0\n",
      "           Conv2d-11         [-1, 16, 142, 142]           4,624\n",
      "             ReLU-12         [-1, 16, 142, 142]               0\n",
      "           Conv2d-13         [-1, 16, 140, 140]           2,320\n",
      "             ReLU-14         [-1, 16, 140, 140]               0\n",
      "           Conv2d-15          [-1, 1, 138, 138]             145\n",
      "================================================================\n",
      "Total params: 53,617\n",
      "Trainable params: 53,617\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 47.07\n",
      "Params size (MB): 0.20\n",
      "Estimated Total Size (MB): 47.36\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder(in_channels=1, downsampling_factors=[downsampling_factor]*model_depth, fmaps=32, fmul=2, kernel_size = 3).to(device)\n",
    "summary(model, (1, 156, 156))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a75eaec-3156-45e0-9033-9c236c2ec496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logdir for each run and a corresponding summary writer\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "writer = SummaryWriter(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56596df7-0a9a-4ced-a71d-0d030928261b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # create train dataset\n",
    "    dataset = EmbryoNucleiDataset(root_dir, crop_size)\n",
    "\n",
    "    # create train dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True, pin_memory=True)\n",
    "\n",
    "    # create model\n",
    "    model = Autoencoder(in_channels=1, downsampling_factors=[downsampling_factor]*model_depth,\n",
    "        fmaps=32, fmul=2, kernel_size = 3)\n",
    "\n",
    "    # create loss object\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "\n",
    "    # create optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_epoch(dataloader, model, epoch, optimizer, loss_function)\n",
    "\n",
    "def train_epoch(dataloader, model, epoch, optimizer, loss_function, log_image_interval = 20):\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    loss_list = []  \n",
    "    \n",
    "    for batch_id, (raw, mask) in enumerate(dataloader):\n",
    "        #raw = torch.from_numpy(raw) # convert to torch tensor\n",
    "        raw = raw.to(device)[0] # move to GPU\n",
    "        #print(f\"raw shape {raw.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        # apply model and calculate loss\n",
    "        _, prediction = model(raw)\n",
    "        reduction = raw.shape[2] - prediction.shape[2]\n",
    "        raw = raw[:, :, reduction//2:-reduction//2, reduction//2:-reduction//2]\n",
    "        loss = loss_function(prediction, raw)\n",
    "        #print(loss.item())\n",
    "        #writer.add_scalar('loss',loss.item(), batch_id)\n",
    "        loss_list.append(loss.item())\n",
    "        # backpropagate the loss and adjust the parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(f\"Len dataset is {len(dataloader)}\")\n",
    "        step = epoch * len(dataloader) + batch_id\n",
    "        if step % log_image_interval == 0:\n",
    "            writer.add_images(\n",
    "                tag=\"input\", img_tensor=raw[16:17].to(\"cpu\"), global_step=step\n",
    "            )\n",
    "            writer.add_images(\n",
    "                tag=\"prediction\",\n",
    "                img_tensor=prediction[16:17].to(\"cpu\").detach(),\n",
    "                global_step=step,\n",
    "            )\n",
    "    loss_list = np.array(loss_list)\n",
    "    print(f\"Loss at Epoch {epoch} is {loss_list.mean()}\")\n",
    "    writer.add_scalar('loss',(loss.cpu().detach().numpy()) *0.001, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02e25b0d-7380-440b-8daf-c7c11f9f1653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▉                                                                                               | 1/50 [00:42<34:43, 42.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Epoch 0 is 0.0011134577820484993\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▉                                                                                             | 2/50 [01:30<36:39, 45.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Epoch 1 is 0.00022380693517334293\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▊                                                                                           | 3/50 [02:16<35:58, 45.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Epoch 2 is 0.00043085340093966805\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▊                                                                                           | 3/50 [02:23<37:28, 47.84s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs)):\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader, model, epoch, optimizer, loss_function, log_image_interval)\u001b[0m\n\u001b[1;32m     24\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m loss_list \u001b[38;5;241m=\u001b[39m []  \n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id, (raw, mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#raw = torch.from_numpy(raw) # convert to torch tensor\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     raw \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mto(device)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# move to GPU\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#print(f\"raw shape {raw.shape}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mEmbryoNucleiDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     67\u001b[0m raw \u001b[38;5;241m=\u001b[39m tifffile\u001b[38;5;241m.\u001b[39mimread(raw_file) \u001b[38;5;66;03m# load raw to numpy array\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#mask = tifffile.imread(mask_file) # load mask to numpy array\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# from (H, W) mask extract (B, h, h)\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m crops_raw, crops_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_masked_crops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#print(f\"Crops raw have shape {crops_raw.shape}. Crops Mask have shape {crops_mask.shape}\")\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# need to cast to float32\u001b[39;00m\n\u001b[1;32m     75\u001b[0m crops_mask \u001b[38;5;241m=\u001b[39m (crops_mask \u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mEmbryoNucleiDataset.get_masked_crops\u001b[0;34m(self, raw, mask)\u001b[0m\n\u001b[1;32m     38\u001b[0m crops_raw \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m crops_mask \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 40\u001b[0m centroids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_centroids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(crops_mask) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m     43\u001b[0m     centroid \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(centroids)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mEmbryoNucleiDataset.get_centroids\u001b[0;34m(self, mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m centroids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m---> 22\u001b[0m     y,x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     xm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(x))\n\u001b[1;32m     24\u001b[0m     ym \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(y))\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train() # add normalize? tensorboard? train for longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c7bb3-f6fb-4406-a843-38e0d6eea8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view runs in tensorboard you can call either (uncommented):\n",
    "%reload_ext tensorboard\n",
    "!tensorboard --logdir logs --port 6009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1df56e-68fb-4b4e-8bbf-0efc20dc9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "[2]*5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516742a-6fbc-4563-b403-63589d6234c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:06_instance_segmentation]",
   "language": "python",
   "name": "conda-env-06_instance_segmentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
