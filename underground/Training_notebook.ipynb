{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "40d37bcf-67ee-4caa-a6ee-0231f04c79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from glob import glob\n",
    "import tifffile\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ad0e8-9bd1-4100-85bf-0af33b592228",
   "metadata": {},
   "source": [
    "### Creating a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "43a192c5-22f2-4194-907b-0995caa1e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbryoNucleiDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 epoch_size\n",
    "                ):\n",
    "        \n",
    "        # using root_dir, split and mask create a path to files and sort it \n",
    "        self.mask_files = sorted(glob(os.path.join(root_dir, 'cropped_masks', '*.tif'))) # load mask files into sorted list\n",
    "        self.raw_files = sorted(glob(os.path.join(root_dir, 'cropped_rawfiles', '*.tif'))) # load image files into sorted list\n",
    "        self.epoch_size = epoch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        #return len(self.raw_files)\n",
    "        return self.epoch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = np.random.randint(len(self.raw_files))\n",
    "        raw_file = self.raw_files[idx] \n",
    "        mask_file = self.mask_files[idx] \n",
    "        crops_raw = tifffile.imread(raw_file) # load raw to numpy array\n",
    "        crops_mask = tifffile.imread(mask_file) # load mask to numpy array\n",
    "        crops_mask = (crops_mask !=0).astype(np.float32)\n",
    "        crops_raw = ((crops_raw.astype(np.float32))/65535) * crops_mask\n",
    "        \n",
    "        # add channel dimensions to comply with pytorch standard (B, C, H, W) \n",
    "        crops_raw = np.expand_dims(crops_raw, axis=0)\n",
    "        crops_mask = np.expand_dims(crops_mask, axis=0)\n",
    "        \n",
    "        return crops_raw, crops_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabeb4b-2073-4e51-9033-cdfb1e8d22ce",
   "metadata": {},
   "source": [
    "### Creating Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "015211c4-1793-4b3c-a578-a68d59e09fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            downsampling_factors,\n",
    "            fmaps,\n",
    "            fmul,\n",
    "            fmaps_bottle = 'default',\n",
    "            kernel_size=3):\n",
    "\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        out_channels = in_channels\n",
    "\n",
    "        encoder = []\n",
    "\n",
    "        for downsampling_factor in downsampling_factors:\n",
    "\n",
    "            encoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "            encoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "            encoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        fmaps,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "            encoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "            encoder.append(\n",
    "                    torch.nn.MaxPool2d(downsampling_factor))\n",
    "\n",
    "            in_channels = fmaps\n",
    "\n",
    "            fmaps = fmaps * fmul\n",
    "\n",
    "        if fmaps_bottle == 'default':\n",
    "            fmaps_bottle = fmaps\n",
    "        \n",
    "        encoder.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels,\n",
    "                fmaps_bottle,\n",
    "                kernel_size))\n",
    "        encoder.append(\n",
    "            torch.nn.ReLU(inplace=True))\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*encoder)\n",
    "\n",
    "        decoder = []\n",
    "\n",
    "        fmaps = in_channels\n",
    "\n",
    "        decoder.append(\n",
    "            torch.nn.Conv2d(\n",
    "                fmaps_bottle,\n",
    "                fmaps,\n",
    "                kernel_size))\n",
    "        decoder.append(\n",
    "            torch.nn.ReLU(inplace=True))\n",
    "\n",
    "        for idx, downsampling_factor in enumerate(downsampling_factors[::-1]):\n",
    "\n",
    "            decoder.append(\n",
    "                torch.nn.Upsample(\n",
    "                    scale_factor=downsampling_factor,\n",
    "                    mode='bilinear'))\n",
    "\n",
    "            in_channels = fmaps\n",
    "            \n",
    "            decoder.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    fmaps,\n",
    "                    kernel_size))\n",
    "            decoder.append(\n",
    "                torch.nn.ReLU(inplace=True))\n",
    "            if idx < len(downsampling_factors) - 1:\n",
    "                fmaps = in_channels // fmul\n",
    "                decoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "                decoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "\n",
    "            else:\n",
    "                decoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size))\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(*decoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        enc = self.encoder(x)\n",
    "\n",
    "        dec = self.decoder(enc)\n",
    "\n",
    "        return enc, dec\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d3687-f7a3-4f31-8a6a-ad950bfc4f8f",
   "metadata": {},
   "source": [
    "### Create training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "56596df7-0a9a-4ced-a71d-0d030928261b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(batch_size,num_epochs,epoch_size):\n",
    "    # create train dataset\n",
    "    dataset = EmbryoNucleiDataset(root_dir,epoch_size)\n",
    "\n",
    "    # create train dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    # create model\n",
    "    model = Autoencoder(in_channels=1, downsampling_factors=[downsampling_factor]*model_depth,\n",
    "        fmaps=32, fmul=2, kernel_size = 3)\n",
    "\n",
    "    # create loss object\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    #loss_function = torch.nn.L1Loss()\n",
    "\n",
    "    # create optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), position=0, leave=True):\n",
    "        train_epoch(dataloader, model, epoch, optimizer, loss_function)\n",
    "\n",
    "def train_epoch(dataloader, model, epoch, optimizer, loss_function, log_image_interval = 20):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    loss_list = []\n",
    "    \n",
    "    for batch_id, (raw, mask) in enumerate(dataloader):\n",
    "        raw = raw.to(device) # move to GPU\n",
    "        optimizer.zero_grad()\n",
    "        _, prediction = model(raw)\n",
    "        reduction = raw.shape[2] - prediction.shape[2]\n",
    "        raw = raw[:, :, reduction//2:-reduction//2, reduction//2:-reduction//2]\n",
    "        loss = loss_function(prediction, raw)\n",
    "        step = epoch * len(dataloader) + batch_id\n",
    "        writer.add_scalar('train loss',loss.item(), step)\n",
    "        loss_list.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % log_image_interval == 0:\n",
    "            writer.add_images(\n",
    "                tag=\"input\", img_tensor=raw.to(\"cpu\"), global_step=step\n",
    "            )\n",
    "            writer.add_images(\n",
    "                tag=\"prediction\",\n",
    "                img_tensor=prediction.to(\"cpu\").detach(),\n",
    "                global_step=step,\n",
    "            )\n",
    "    loss_list = np.array(loss_list)\n",
    "    print(f\"Loss at Epoch {epoch} is {loss_list.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d250cf0-04ac-4065-9776-fc2cc0bea19a",
   "metadata": {},
   "source": [
    "### Training Time ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "86f9571c-b030-413b-aa2e-b10f2110a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying params for training\n",
    "batch_size = 64\n",
    "crop_size = 156\n",
    "num_epochs = 10\n",
    "epoch_size = 5000\n",
    "root_dir = '/mnt/efs/shared_data/instance_no_gt/20230830_TIF_cellpose_test/'\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "35176614-4dbb-42ca-ade7-f5ba5e37a2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 2, 154, 154]              20\n",
      "              ReLU-2          [-1, 2, 154, 154]               0\n",
      "            Conv2d-3          [-1, 2, 152, 152]              38\n",
      "              ReLU-4          [-1, 2, 152, 152]               0\n",
      "         MaxPool2d-5            [-1, 2, 76, 76]               0\n",
      "            Conv2d-6            [-1, 4, 74, 74]              76\n",
      "              ReLU-7            [-1, 4, 74, 74]               0\n",
      "            Conv2d-8            [-1, 2, 72, 72]              74\n",
      "              ReLU-9            [-1, 2, 72, 72]               0\n",
      "         Upsample-10          [-1, 2, 144, 144]               0\n",
      "           Conv2d-11          [-1, 2, 142, 142]              38\n",
      "             ReLU-12          [-1, 2, 142, 142]               0\n",
      "           Conv2d-13          [-1, 1, 140, 140]              19\n",
      "================================================================\n",
      "Total params: 265\n",
      "Trainable params: 265\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 3.09\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 3.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_depth = 1\n",
    "downsampling_factor = 2\n",
    "downsampling_factors = [downsampling_factor]*model_depth\n",
    "fmaps = 2\n",
    "fmul = 2\n",
    "fmaps_bottle = 'default'\n",
    "kernel_size = 3\n",
    "loss = 'MSE'\n",
    "\n",
    "model = Autoencoder(in_channels=1, downsampling_factors=downsampling_factors, fmaps=fmaps,\n",
    "                    fmul=fmul, fmaps_bottle = fmaps_bottle, kernel_size = kernel_size).to(device)\n",
    "summary(model, (1, 156, 156))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2a75eaec-3156-45e0-9033-9c236c2ec496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logdir for each run and a corresponding summary writer\n",
    "train_identifier = f'FINALTESTautoencoder_downsamplingfactors_{downsampling_factors}__fmaps_{fmaps}__fmul_{fmul}__fmapsbottle_{fmaps_bottle}__kernelsize_{kernel_size}__loss_{loss}'\n",
    "logdir = os.path.join(\"logs\", f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}')\n",
    "writer = SummaryWriter(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e25b0d-7380-440b-8daf-c7c11f9f1653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 1/10 [00:47<07:08, 47.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Epoch 0 is 0.0002143297482044672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 2/10 [01:35<06:22, 47.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Epoch 1 is 3.602980916783261e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████▏                              | 3/10 [02:23<05:34, 47.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Epoch 2 is 2.3179328343735216e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▌                          | 4/10 [03:10<04:44, 47.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Epoch 3 is 1.5523930299803142e-05\n"
     ]
    }
   ],
   "source": [
    "train(batch_size=batch_size,num_epochs=num_epochs,epoch_size=epoch_size) # tensorboard? train for longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6c7bb3-f6fb-4406-a843-38e0d6eea8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "/home/evan/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.29' not found (required by /home/evan/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/evan/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /home/evan/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/evan/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28' not found (required by /home/evan/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/evan/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /home/evan/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/evan/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /home/evan/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "Address already in use\n",
      "Port 6009 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
     ]
    }
   ],
   "source": [
    "# To view runs in tensorboard you can call either (uncommented):\n",
    "%reload_ext tensorboard\n",
    "!tensorboard --logdir logs --port 6009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f516742a-6fbc-4563-b403-63589d6234c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model weights\n",
    "state = model.state_dict()\n",
    "filename = root_dir+'models/'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'.pt'\n",
    "torch.save(state, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545adbb-890e-403c-ab25-b41ba9b12ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2a19111-65d6-4c23-b785-56f90a636689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test: \n",
    "# Model depth, L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e5e79-f881-44a5-855d-be65633c2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate: \n",
    "# IOU (segmentation performance), Pearson (reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30813c-4519-40e7-9032-083b2b0b91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761480a-68b2-422a-aadb-d9506ec093b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOBIE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bfa881-9316-4b67-ab63-a2101232ddc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:06_instance_segmentation]",
   "language": "python",
   "name": "conda-env-06_instance_segmentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
