{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d37bcf-67ee-4caa-a6ee-0231f04c79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from glob import glob\n",
    "import tifffile\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ad0e8-9bd1-4100-85bf-0af33b592228",
   "metadata": {},
   "source": [
    "### Creating a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a192c5-22f2-4194-907b-0995caa1e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbryoNucleiDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 epoch_size\n",
    "                ):\n",
    "        \n",
    "        # using root_dir, split and mask create a path to files and sort it \n",
    "        self.mask_files = sorted(glob(os.path.join(root_dir, 'cropped_masks', '*.tif'))) # load mask files into sorted list\n",
    "        self.raw_files = sorted(glob(os.path.join(root_dir, 'cropped_rawfiles', '*.tif'))) # load image files into sorted list\n",
    "        self.epoch_size = epoch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        #return len(self.raw_files)\n",
    "        return self.epoch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = np.random.randint(len(self.raw_files))\n",
    "        raw_file = self.raw_files[idx] \n",
    "        mask_file = self.mask_files[idx] \n",
    "        crops_raw = tifffile.imread(raw_file) # load raw to numpy array\n",
    "        crops_mask = tifffile.imread(mask_file) # load mask to numpy array\n",
    "        crops_mask = (crops_mask !=0).astype(np.float32)\n",
    "        crops_raw = ((crops_raw.astype(np.float32))/65535) * crops_mask\n",
    "        \n",
    "        # add channel dimensions to comply with pytorch standard (B, C, H, W) \n",
    "        crops_raw = np.expand_dims(crops_raw, axis=0)\n",
    "        crops_mask = np.expand_dims(crops_mask, axis=0)\n",
    "        \n",
    "        return crops_raw, crops_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabeb4b-2073-4e51-9033-cdfb1e8d22ce",
   "metadata": {},
   "source": [
    "### Creating Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015211c4-1793-4b3c-a578-a68d59e09fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            downsampling_factors,\n",
    "            fmaps,\n",
    "            fmul,\n",
    "            fmaps_bottle = 'default',\n",
    "            kernel_size=3):\n",
    "\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        out_channels = in_channels\n",
    "\n",
    "        encoder = []\n",
    "\n",
    "        for downsampling_factor in downsampling_factors:\n",
    "\n",
    "            encoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "            encoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "            encoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        fmaps,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "            encoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "            encoder.append(\n",
    "                    torch.nn.MaxPool2d(downsampling_factor))\n",
    "\n",
    "            in_channels = fmaps\n",
    "\n",
    "            fmaps = fmaps * fmul\n",
    "\n",
    "        if fmaps_bottle == 'default':\n",
    "            fmaps_bottle = fmaps\n",
    "        \n",
    "        encoder.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels,\n",
    "                fmaps_bottle,\n",
    "                kernel_size))\n",
    "        encoder.append(\n",
    "            torch.nn.ReLU(inplace=True))\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*encoder)\n",
    "\n",
    "        decoder = []\n",
    "\n",
    "        fmaps = in_channels\n",
    "\n",
    "        decoder.append(\n",
    "            torch.nn.Conv2d(\n",
    "                fmaps_bottle,\n",
    "                fmaps,\n",
    "                kernel_size))\n",
    "        decoder.append(\n",
    "            torch.nn.ReLU(inplace=True))\n",
    "\n",
    "        for idx, downsampling_factor in enumerate(downsampling_factors[::-1]):\n",
    "\n",
    "            decoder.append(\n",
    "                torch.nn.Upsample(\n",
    "                    scale_factor=downsampling_factor,\n",
    "                    mode='bilinear'))\n",
    "\n",
    "            in_channels = fmaps\n",
    "            \n",
    "            decoder.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    fmaps,\n",
    "                    kernel_size))\n",
    "            decoder.append(\n",
    "                torch.nn.ReLU(inplace=True))\n",
    "            if idx < len(downsampling_factors) - 1:\n",
    "                fmaps = in_channels // fmul\n",
    "                decoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "                decoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "\n",
    "            else:\n",
    "                decoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size))\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(*decoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        enc = self.encoder(x)\n",
    "\n",
    "        dec = self.decoder(enc)\n",
    "\n",
    "        return enc, dec\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d3687-f7a3-4f31-8a6a-ad950bfc4f8f",
   "metadata": {},
   "source": [
    "### Create training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56596df7-0a9a-4ced-a71d-0d030928261b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(batch_size,num_epochs,epoch_size,loss_function):\n",
    "    # create train dataset\n",
    "    dataset = EmbryoNucleiDataset(root_dir,epoch_size)\n",
    "\n",
    "    # create train dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    # create model\n",
    "    model = Autoencoder(in_channels=1, downsampling_factors=[downsampling_factor]*model_depth,\n",
    "        fmaps=32, fmul=2, kernel_size = 3)\n",
    "\n",
    "    # create loss object\n",
    "    if loss_function == 'MSE':\n",
    "        loss_function = torch.nn.MSELoss()\n",
    "    elif loss_function == 'L1':\n",
    "        loss_function = torch.nn.L1Loss()\n",
    "    else:\n",
    "        print('Invalid loss function')\n",
    "        return _\n",
    "\n",
    "    # create optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), position=0, leave=True):\n",
    "        train_epoch(dataloader, model, epoch, optimizer, loss_function)\n",
    "\n",
    "def train_epoch(dataloader, model, epoch, optimizer, loss_function, log_image_interval = 20):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    loss_list = []\n",
    "    \n",
    "    for batch_id, (raw, mask) in enumerate(dataloader):\n",
    "        raw = raw.to(device) # move to GPU\n",
    "        optimizer.zero_grad()\n",
    "        _, prediction = model(raw)\n",
    "        reduction = raw.shape[2] - prediction.shape[2]\n",
    "        raw = raw[:, :, reduction//2:-reduction//2, reduction//2:-reduction//2]\n",
    "        loss = loss_function(prediction, raw)\n",
    "        step = epoch * len(dataloader) + batch_id\n",
    "        writer.add_scalar('train loss',loss.item(), step)\n",
    "        loss_list.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % log_image_interval == 0:\n",
    "            writer.add_images(\n",
    "                tag=\"input\", img_tensor=raw.to(\"cpu\"), global_step=step\n",
    "            )\n",
    "            writer.add_images(\n",
    "                tag=\"prediction\",\n",
    "                img_tensor=prediction.to(\"cpu\").detach(),\n",
    "                global_step=step,\n",
    "            )\n",
    "    loss_list = np.array(loss_list)\n",
    "    #print(f\"Loss at Epoch {epoch} is {loss_list.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d250cf0-04ac-4065-9776-fc2cc0bea19a",
   "metadata": {},
   "source": [
    "### Training Time ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f9571c-b030-413b-aa2e-b10f2110a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying params for training\n",
    "batch_size = 64\n",
    "crop_size = 156\n",
    "num_epochs = 100\n",
    "epoch_size = 10000\n",
    "root_dir = '/mnt/efs/shared_data/instance_no_gt/20230830_TIF_cellpose_test/'\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35176614-4dbb-42ca-ade7-f5ba5e37a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_depth = 1\n",
    "downsampling_factor = 4\n",
    "downsampling_factors = [downsampling_factor]*model_depth\n",
    "fmaps = 2\n",
    "fmul = 2\n",
    "fmaps_bottle = 'default'\n",
    "kernel_size = 3\n",
    "loss = 'MSE'\n",
    "\n",
    "model = Autoencoder(in_channels=1, downsampling_factors=downsampling_factors, fmaps=fmaps,\n",
    "                    fmul=fmul, fmaps_bottle = fmaps_bottle, kernel_size = kernel_size).to(device)\n",
    "#summary(model, (1, 156, 156))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a75eaec-3156-45e0-9033-9c236c2ec496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logdir for each run and a corresponding summary writer\n",
    "train_identifier = f'LONGTRAININGautoencoder_downsamplingfactors_{downsampling_factors}__fmaps_{fmaps}__fmul_{fmul}__fmapsbottle_{fmaps_bottle}__kernelsize_{kernel_size}__loss_{loss}'\n",
    "logdir = os.path.join(\"logs\", f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}')\n",
    "writer = SummaryWriter(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "02e25b0d-7380-440b-8daf-c7c11f9f1653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [1:51:10<00:00, 66.71s/it]\n"
     ]
    }
   ],
   "source": [
    "train(batch_size=batch_size,num_epochs=num_epochs,epoch_size=epoch_size,loss_function=loss) # tensorboard? train for longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "f516742a-6fbc-4563-b403-63589d6234c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model weights\n",
    "state = model.state_dict()\n",
    "filename = os.path.join(root_dir,'models/',f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}'+'.pt')\n",
    "torch.save(state, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47535afd-a505-4fa0-ab27-31367da062b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [1:36:52<00:00, 58.13s/it]\n"
     ]
    }
   ],
   "source": [
    "loss = 'L1'\n",
    "model = Autoencoder(in_channels=1, downsampling_factors=downsampling_factors, fmaps=fmaps,\n",
    "                    fmul=fmul, fmaps_bottle = fmaps_bottle, kernel_size = kernel_size).to(device)\n",
    "\n",
    "train_identifier = f'LONGTRAININGautoencoder_downsamplingfactors_{downsampling_factors}__fmaps_{fmaps}__fmul_{fmul}__fmapsbottle_{fmaps_bottle}__kernelsize_{kernel_size}__loss_{loss}'\n",
    "logdir = os.path.join(\"logs\", f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}')\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "train(batch_size=batch_size,num_epochs=num_epochs,epoch_size=epoch_size,loss_function=loss) # tensorboard? train for longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a838b7c-a9fd-47b4-bc35-4faa9bdd7660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model weights\n",
    "state = model.state_dict()\n",
    "filename = os.path.join(root_dir,'models/',f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}'+'.pt')\n",
    "torch.save(state, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1553cc98-c829-4986-ad11-0ea0a1b67fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [1:32:11<00:00, 55.31s/it]\n"
     ]
    }
   ],
   "source": [
    "loss = 'MSE'\n",
    "model_depth = 2\n",
    "downsampling_factor = 4\n",
    "downsampling_factors = [downsampling_factor]*model_depth\n",
    "model = Autoencoder(in_channels=1, downsampling_factors=downsampling_factors, fmaps=fmaps,\n",
    "                    fmul=fmul, fmaps_bottle = fmaps_bottle, kernel_size = kernel_size).to(device)\n",
    "\n",
    "train_identifier = f'LONGTRAININGautoencoder_downsamplingfactors_{downsampling_factors}__fmaps_{fmaps}__fmul_{fmul}__fmapsbottle_{fmaps_bottle}__kernelsize_{kernel_size}__loss_{loss}'\n",
    "logdir = os.path.join(\"logs\", f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}')\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "train(batch_size=batch_size,num_epochs=num_epochs,epoch_size=epoch_size,loss_function=loss) # tensorboard? train for longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c9685b6-08a2-484c-b993-1f79080beb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model weights\n",
    "state = model.state_dict()\n",
    "filename = os.path.join(root_dir,'models/',f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}'+'.pt')\n",
    "torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87e9bc77-0436-462c-b273-8179472abc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [1:30:32<00:00, 54.33s/it]\n"
     ]
    }
   ],
   "source": [
    "loss = 'L1'\n",
    "model = Autoencoder(in_channels=1, downsampling_factors=downsampling_factors, fmaps=fmaps,\n",
    "                    fmul=fmul, fmaps_bottle = fmaps_bottle, kernel_size = kernel_size).to(device)\n",
    "\n",
    "train_identifier = f'LONGTRAININGautoencoder_downsamplingfactors_{downsampling_factors}__fmaps_{fmaps}__fmul_{fmul}__fmapsbottle_{fmaps_bottle}__kernelsize_{kernel_size}__loss_{loss}'\n",
    "logdir = os.path.join(\"logs\", f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}')\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "train(batch_size=batch_size,num_epochs=num_epochs,epoch_size=epoch_size,loss_function=loss) # tensorboard? train for longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b3c5802-c32c-46a4-a0b2-cd5a93f254b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model weights\n",
    "state = model.state_dict()\n",
    "filename = os.path.join(root_dir,'models/',f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}'+'.pt')\n",
    "torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d899508-dfed-4067-9c23-e12d720e2382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████          | 75/100 [1:16:03<25:54, 62.18s/it]"
     ]
    }
   ],
   "source": [
    "loss = 'MSE'\n",
    "model_depth = 3\n",
    "downsampling_factor = 2\n",
    "downsampling_factors = [downsampling_factor]*model_depth\n",
    "model = Autoencoder(in_channels=1, downsampling_factors=downsampling_factors, fmaps=fmaps,\n",
    "                    fmul=fmul, fmaps_bottle = fmaps_bottle, kernel_size = kernel_size).to(device)\n",
    "\n",
    "train_identifier = f'LONGTRAININGautoencoder_downsamplingfactors_{downsampling_factors}__fmaps_{fmaps}__fmul_{fmul}__fmapsbottle_{fmaps_bottle}__kernelsize_{kernel_size}__loss_{loss}'\n",
    "logdir = os.path.join(\"logs\", f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}')\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "train(batch_size=batch_size,num_epochs=num_epochs,epoch_size=epoch_size,loss_function=loss) # tensorboard? train for longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c7736-5952-495f-b6cd-1c22721bb0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model weights\n",
    "state = model.state_dict()\n",
    "filename = os.path.join(root_dir,'models/',f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}'+'.pt')\n",
    "torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629a327-37b3-4e61-b9da-2ba9e6a651af",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'L1'\n",
    "model = Autoencoder(in_channels=1, downsampling_factors=downsampling_factors, fmaps=fmaps,\n",
    "                    fmul=fmul, fmaps_bottle = fmaps_bottle, kernel_size = kernel_size).to(device)\n",
    "\n",
    "train_identifier = f'LONGTRAININGautoencoder_downsamplingfactors_{downsampling_factors}__fmaps_{fmaps}__fmul_{fmul}__fmapsbottle_{fmaps_bottle}__kernelsize_{kernel_size}__loss_{loss}'\n",
    "logdir = os.path.join(\"logs\", f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}')\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "train(batch_size=batch_size,num_epochs=num_epochs,epoch_size=epoch_size,loss_function=loss) # tensorboard? train for longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6319a7-b817-420c-99e2-2a633988b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model weights\n",
    "state = model.state_dict()\n",
    "filename = os.path.join(root_dir,'models/',f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}_{train_identifier}'+'.pt')\n",
    "torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ff6c7bb3-f6fb-4406-a843-38e0d6eea8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "/home/brunou/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.29' not found (required by /home/brunou/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/brunou/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /home/brunou/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/brunou/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28' not found (required by /home/brunou/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/brunou/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /home/brunou/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/brunou/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /home/brunou/conda/envs/06_instance_segmentation/lib/python3.8/site-packages/tensorboard_data_server/bin/server)\n",
      "Address already in use\n",
      "Port 6009 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
     ]
    }
   ],
   "source": [
    "# To view runs in tensorboard you can call either (uncommented):\n",
    "%reload_ext tensorboard\n",
    "!tensorboard --logdir logs --port 6009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545adbb-890e-403c-ab25-b41ba9b12ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e5e79-f881-44a5-855d-be65633c2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate: \n",
    "# IOU (segmentation performance), Pearson (reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30813c-4519-40e7-9032-083b2b0b91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761480a-68b2-422a-aadb-d9506ec093b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOBIE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bfa881-9316-4b67-ab63-a2101232ddc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:06_instance_segmentation]",
   "language": "python",
   "name": "conda-env-06_instance_segmentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
