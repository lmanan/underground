{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5eb89e0-e103-4e15-bfab-1dc9d7ae57f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from glob import glob\n",
    "import tifffile\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826918cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbryoNucleiDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 epoch_size\n",
    "                ):\n",
    "        \n",
    "        # using root_dir, split and mask create a path to files and sort it \n",
    "        self.mask_files = sorted(glob(os.path.join(root_dir, 'cropped_masks', '*.tif'))) # load mask files into sorted list\n",
    "        self.raw_files = sorted(glob(os.path.join(root_dir, 'cropped_rawfiles', '*.tif'))) # load image files into sorted list\n",
    "        self.epoch_size = epoch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        #return len(self.raw_files)\n",
    "        return self.epoch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = np.random.randint(len(self.raw_files))\n",
    "        raw_file = self.raw_files[idx] \n",
    "        mask_file = self.mask_files[idx] \n",
    "        crops_raw = tifffile.imread(raw_file) # load raw to numpy array\n",
    "        crops_mask = tifffile.imread(mask_file) # load mask to numpy array\n",
    "        crops_mask = (crops_mask !=0).astype(np.float32)\n",
    "        crops_raw = ((crops_raw.astype(np.float32))/65535) * crops_mask\n",
    "        \n",
    "        # add channel dimensions to comply with pytorch standard (B, C, H, W) \n",
    "        crops_raw = np.expand_dims(crops_raw, axis=0)\n",
    "        crops_mask = np.expand_dims(crops_mask, axis=0)\n",
    "        \n",
    "        return crops_raw, crops_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ff794-1913-4247-8875-d995a56869a6",
   "metadata": {},
   "source": [
    "## Extracting the latent representation of all objects (N=371,012)\n",
    "Be sure to match all Autoencoder and dataloader params below to match that used in the previous training (found in Training_notebook.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498e695e-e947-486b-a49e-fc21ad9bcd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            downsampling_factors,\n",
    "            fmaps,\n",
    "            fmul,\n",
    "            fmaps_bottle = 'default',\n",
    "            kernel_size=3):\n",
    "\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        out_channels = in_channels\n",
    "\n",
    "        encoder = []\n",
    "\n",
    "        for downsampling_factor in downsampling_factors:\n",
    "\n",
    "            encoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "            encoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "            encoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        fmaps,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "            encoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "            encoder.append(\n",
    "                    torch.nn.MaxPool2d(downsampling_factor))\n",
    "\n",
    "            in_channels = fmaps\n",
    "\n",
    "            fmaps = fmaps * fmul\n",
    "\n",
    "        if fmaps_bottle == 'default':\n",
    "            fmaps_bottle = fmaps\n",
    "        \n",
    "        encoder.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels,\n",
    "                fmaps_bottle,\n",
    "                kernel_size))\n",
    "        encoder.append(\n",
    "            torch.nn.ReLU(inplace=True))\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*encoder)\n",
    "\n",
    "        decoder = []\n",
    "\n",
    "        fmaps = in_channels\n",
    "\n",
    "        decoder.append(\n",
    "            torch.nn.Conv2d(\n",
    "                fmaps_bottle,\n",
    "                fmaps,\n",
    "                kernel_size))\n",
    "        decoder.append(\n",
    "            torch.nn.ReLU(inplace=True))\n",
    "\n",
    "        for idx, downsampling_factor in enumerate(downsampling_factors[::-1]):\n",
    "\n",
    "            decoder.append(\n",
    "                torch.nn.Upsample(\n",
    "                    scale_factor=downsampling_factor,\n",
    "                    mode='bilinear'))\n",
    "\n",
    "            in_channels = fmaps\n",
    "            \n",
    "            decoder.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    fmaps,\n",
    "                    kernel_size))\n",
    "            decoder.append(\n",
    "                torch.nn.ReLU(inplace=True))\n",
    "            if idx < len(downsampling_factors) - 1:\n",
    "                fmaps = in_channels // fmul\n",
    "                decoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        fmaps,\n",
    "                        kernel_size))\n",
    "                decoder.append(\n",
    "                    torch.nn.ReLU(inplace=True))\n",
    "\n",
    "            else:\n",
    "                decoder.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size))\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(*decoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        enc = self.encoder(x)\n",
    "\n",
    "        dec = self.decoder(enc)\n",
    "\n",
    "        return enc, dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d059b",
   "metadata": {},
   "source": [
    "### Be careful while setting these values below. They should correspond to the desired model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82044800",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")\n",
    "model_depth = 1\n",
    "downsampling_factor = 4\n",
    "downsampling_factors = [downsampling_factor]*model_depth\n",
    "fmaps = 2\n",
    "fmul = 2\n",
    "fmaps_bottle = 'default'\n",
    "kernel_size = 3\n",
    "\n",
    "\n",
    "model = Autoencoder(in_channels=1, downsampling_factors=downsampling_factors, fmaps=fmaps,\n",
    "                    fmul=fmul, fmaps_bottle = fmaps_bottle, kernel_size = kernel_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0cb30-a333-4b12-a918-236d7e85e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directories\n",
    "root_dir = '/mnt/efs/shared_data/instance_no_gt/20230830_TIF_cellpose_test/'\n",
    "model_name = 'models/20230903-023210_LONGTRAININGautoencoder_downsamplingfactors_[4]__fmaps_2__fmul_2__fmapsbottle_default__kernelsize_3__loss_MSE.pt'\n",
    "# Importing the model\n",
    "model = Autoencoder(in_channels=1, downsampling_factors=[downsampling_factor]*model_depth, fmaps=fmaps, fmul=fmul, kernel_size = kernel_size).to(device)\n",
    "state = torch.load(root_dir+model_name)\n",
    "model.load_state_dict(state, strict=True)\n",
    "#os.mkdir(os.path.join(root_dir, 'latent_spaces', os.path.basename(model_name[:-3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a88d2b56-0f32-44e2-a783-9371d7042e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the file names of raw and mask files\n",
    "raw_files = sorted(glob(os.path.join(root_dir, 'cropped_rawfiles', '*.tif'))) \n",
    "mask_files = sorted(glob(os.path.join(root_dir, 'cropped_masks', '*.tif')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8acfdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train dataset\n",
    "dataset = EmbryoNucleiDataset(root_dir,epoch_size=len(raw_files))\n",
    "\n",
    "# create train dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9bf35-5c10-4bb9-bb3d-c8569b20296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping the pre-trained model over the entire dataset: \n",
    "\n",
    "# make sure net is in eval mode so we don't backprop\n",
    "model.eval()\n",
    "\n",
    "for idx, (raw, mask) in enumerate(tqdm(dataloader)):\n",
    "    raw = raw.to(device) \n",
    "    mask = mask.to(device)\n",
    "    latent,_ = model(raw) # key line\n",
    "    latent = latent[0].cpu().detach().numpy()\n",
    "    filename = os.path.basename(raw_files[idx][:-4])+'.npy'\n",
    "    np.save(os.path.join(root_dir, 'latent_spaces', os.path.basename(model_name[:-3]), filename), latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a01bb-b676-4b32-80f1-7e2f47e2bb58",
   "metadata": {},
   "source": [
    "## Representing the latent space with UMAP plotting\n",
    "Question: do learned features cluster in potentially meaningful ways?\n",
    "\n",
    "What we have: a directory of 371,012 latent representations of datatype float32 and shape (1, 64, 74, 74)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0c244-7102-46ef-97e1-e7594d52c136",
   "metadata": {},
   "source": [
    "### Testing if things are installed correctly (the Penguin Test)\n",
    "Previous to this, I had followed instructions to install this in the conda env 06_instance_segmentation (via Jupyter terminal).\n",
    "See: https://umap-learn.readthedocs.io/en/latest/index.html\n",
    "\n",
    "We need to install seaborn, umap, pandas, and make sure sklearn is version 1.3.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c401d-8b57-4095-8646-9a51793e8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd7b70-74c9-4014-9dae-bbd4c08d88fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import pandas as pd \n",
    "%matplotlib inline\n",
    "\n",
    "# needed to install umap, seaborn, pandas before running these cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242a4ea-cc45-4920-affd-cf02b60148a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/c19a904462482430170bfe2c718775ddb7dbb885/inst/extdata/penguins.csv\")\n",
    "penguins.head()\n",
    "\n",
    "penguins = penguins.dropna()\n",
    "penguins.species.value_counts()\n",
    "\n",
    "sns.pairplot(penguins.drop(\"year\", axis=1), hue='species');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d34ee5-fea4-48b7-85aa-868d3ad14244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sklearn version number! if it is NOT 1.3.0, need to pip uninstall scikit-learn and pip install scikit-learn==1.3.0\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25648a4-dafe-45ca-8a2f-d33b8eed012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import DistanceMetric # needed to uninstall scikit-learn and  pip install scikit-learn==1.3.0 for this import to work\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc271d2-f0aa-42bd-804e-c9345efaad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "\n",
    "penguin_data = penguins[\n",
    "    [\n",
    "        \"bill_length_mm\",\n",
    "        \"bill_depth_mm\",\n",
    "        \"flipper_length_mm\",\n",
    "        \"body_mass_g\",\n",
    "    ]\n",
    "].values\n",
    "scaled_penguin_data = StandardScaler().fit_transform(penguin_data)\n",
    "\n",
    "embedding = reducer.fit_transform(scaled_penguin_data)\n",
    "embedding.shape\n",
    "\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c=[sns.color_palette()[x] for x in penguins.species.map({\"Adelie\":0, \"Chinstrap\":1, \"Gentoo\":2})])\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "\n",
    "plt.title('UMAP projection of the Penguin dataset', fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0a353-0eae-41c1-b75a-3e4c8aecf41c",
   "metadata": {},
   "source": [
    "### UMAP for our latent representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5822e32-3a5c-4888-b564-38e3c4392ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ffa40-c47c-42ac-87e6-affe7f9cfe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing test data (n=563)\n",
    "latent_dir = '/mnt/efs/shared_data/instance_no_gt/20230830_TIF_cellpose_test/latent_spaces/'\n",
    "latent_files = sorted(glob(os.path.join(latent_dir, '*.tif')))\n",
    "len(latent_files) # length is 563 as we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e0584b-109b-4866-a73e-6808b1ba7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding a low dimensional representation of the data.\n",
    "fit = umap.UMAP()\n",
    "%time u = fit.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d380770-5150-4345-ac5a-92b1c7677358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latent_files[1])\n",
    "test_image = tifffile.imread(latent_files[0])\n",
    "#test_image = test_image[1:]\n",
    "#test_image.shape # (1, 64, 74, 74)\n",
    "test_image = np.array(test_image)\n",
    "test_image.shape\n",
    "#test_image.dtype # float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d246d9-b4ae-41a8-b8de-70beca34ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = []\n",
    "\n",
    "for image_file in latent_files:\n",
    "    # read into image\n",
    "    image = tifffile.imread(image_file)\n",
    "    \n",
    "    # Flatten each image into a 1D vector and stack them vertically\n",
    "    reshaped_image = image.flatten()\n",
    "\n",
    "    latents.append(reshaped_image)\n",
    "    \n",
    "latents = np.stack(latents) # 562 times number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d6a9e-2f5d-445d-a136-09a66dacd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = umap.UMAP()\n",
    "u = fit.fit_transform(latents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1a880-c0bc-4286-b439-5e2bd1cfd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(u[:,0], u[:,1])\n",
    "plt.title('UMAP embedding of random colours');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ba0a3-8313-4995-92cf-72814a2c0d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
